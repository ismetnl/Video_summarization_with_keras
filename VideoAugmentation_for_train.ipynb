{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJxAJjeInSST"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoqmwFVFnrpb"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/okankop/vidaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBB_FD3zYnCk"
      },
      "outputs": [],
      "source": [
        "!pip install keras-layer-normalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "FcYRAcq_fX8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.losses import CosineSimilarity"
      ],
      "metadata": {
        "id": "1YQvTBZSfR46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbEMVl9FYrcH"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from keras import backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.layers import Input, Dense, Flatten, Activation, Dropout, Bidirectional, Permute, multiply\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.callbacks import CSVLogger\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.xception import Xception\n",
        "#from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import Conv2DTranspose, ConvLSTM2D, BatchNormalization, TimeDistributed, Conv2D\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Flatten, Bidirectional, Dense, Permute, multiply,Dropout, LayerNormalization,GaussianNoise, GaussianDropout ,GlobalAveragePooling1D,BatchNormalization\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "import numpy as np\n",
        "import h5py\n",
        "import math\n",
        "from pandas import read_json\n",
        "import torch\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiavXrOon6dN"
      },
      "outputs": [],
      "source": [
        "from vidaug import augmentors as va\n",
        "import cv2\n",
        "from PIL import Image, ImageSequence\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "import h5py\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import shutil\n",
        "import cv2\n",
        "import os\n",
        "from numba import jit\n",
        "import math\n",
        "from pandas import read_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_OVtGoMXIQ7"
      },
      "outputs": [],
      "source": [
        "def load_video(video_path):\n",
        "\n",
        "  frames = []\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  \n",
        "\n",
        "  for frame_idx in range(n_frames-1):\n",
        "    success, frame = cap.read()\n",
        "    \n",
        "    if success:\n",
        "      if frame_idx % 10 == 0:\n",
        "        frames.append(frame)\n",
        "\n",
        "\n",
        "  return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA2_cZN7w1g0"
      },
      "outputs": [],
      "source": [
        "def videoAugmentation(frames,aug_type):\n",
        "  \n",
        "  seq = None\n",
        "\n",
        "  if aug_type == \"HorizontalFlip\" :\n",
        "    seq = va.Sequential([ \n",
        "      va.HorizontalFlip(), \n",
        "    ])\n",
        "\n",
        "  elif aug_type == \"VerticalFlip\" :\n",
        "    seq = va.Sequential([ \n",
        "        va.VerticalFlip(),\n",
        "    ])\n",
        "\n",
        "  elif aug_type == \"InvertColor\" :\n",
        "    seq = va.Sequential([ \n",
        "        va.InvertColor(),\n",
        "    ])\n",
        "  elif aug_type == \"Salt\" :\n",
        "    seq = va.Sequential([ \n",
        "        va.Salt(),\n",
        "    ])\n",
        "  elif aug_type == \"Pepper\" :\n",
        "    seq = va.Sequential([ \n",
        "        va.Pepper(),\n",
        "    ])\n",
        "\n",
        "  else:\n",
        "    seq = va.Sequential([ \n",
        "        va.Add(),\n",
        "    ])\n",
        "  \n",
        "\n",
        "  video_aug = seq(frames)\n",
        "\n",
        "  return video_aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGp64lHW1_Md"
      },
      "outputs": [],
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = tensorflow.keras.applications.VGG19(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(224, 224, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.vgg19.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((224, 224, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9NiFIvTC3kC"
      },
      "outputs": [],
      "source": [
        "def extract_feature(feature_extractor, frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "    frame = frame[None, ...]\n",
        "    feature = feature_extractor.predict(frame)\n",
        "    frame_feature = feature.flatten()\n",
        "\n",
        "    return feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o27VKjJuJsTa"
      },
      "outputs": [],
      "source": [
        "files = glob.glob(\"/content/drive/MyDrive/summe/videos\"+\"/*.mp4\", recursive = True)\n",
        "files.sort()\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWSc-QsbPJPH"
      },
      "outputs": [],
      "source": [
        "SPLIT = \"/content/drive/MyDrive/summe/summe_splits.json\"\n",
        "SPLIT_ID = 0\n",
        "splits = read_json(SPLIT)\n",
        "train_keys = splits[\"train_keys\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9UgJ2GXRN89"
      },
      "outputs": [],
      "source": [
        "def get_video_paths(train_keys):\n",
        "  video_paths_for_train = []\n",
        "  for keys in train_keys:\n",
        "    video_paths_for_train.append(files[int(keys.split(\"video_\")[1]) - 1])\n",
        "  return video_paths_for_train\n",
        "train_path = get_video_paths(train_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0Dti2AGTsdo",
        "outputId": "0286ae87-0cfa-4f2d-b279-adea463869b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ],
      "source": [
        "def prepare_videos(video_paths, aug_types ,feature_extractor):\n",
        "  \n",
        "  count = 0\n",
        "  video_feat_for_train = None\n",
        "  video_features = []\n",
        "  video_aug = None\n",
        "\n",
        "  for path in video_paths:\n",
        "    frames = load_video(path)\n",
        "\n",
        "    for frame in frames:\n",
        "      frame_feat = extract_feature(feature_extractor,frame)\n",
        "\n",
        "      if video_feat_for_train is None:\n",
        "          video_feat_for_train = frame_feat\n",
        "      else:\n",
        "          video_feat_for_train = np.vstack((video_feat_for_train, frame_feat))\n",
        "    \n",
        "    video_features.append(video_feat_for_train)\n",
        "\n",
        "    for aug in aug_types:\n",
        "      frames_aug = videoAugmentation(frames, aug)\n",
        "\n",
        "      for frame in frames_aug:\n",
        "        frame_feat = extract_feature(feature_extractor,frame)\n",
        "\n",
        "        if video_aug is None:\n",
        "            video_aug = frame_feat\n",
        "        else:\n",
        "            video_aug = np.vstack((video_aug, frame_feat))\n",
        "      video_features.append(video_aug)\n",
        "      video_aug = None\n",
        "    video_feat_for_train = None\n",
        "\n",
        "  with open('/content/drive/MyDrive/Data_aug_for_sum.npy', 'wb') as f:\n",
        "    np.save(f, video_features)\n",
        "  \n",
        "  return video_features\n",
        "      \n",
        "aug_types = [\"Salt\"]\n",
        "video_features = prepare_videos(train_path ,aug_types, feature_extractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OejpysmMgUvB"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "  BATCH_SIZE = 1\n",
        "  EPOCHS = 60\n",
        "  MODEL_PATH = \"/content/drive/MyDrive/summe/models/summary_2_salt_model.hdf5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIqMiCj8kmAl"
      },
      "outputs": [],
      "source": [
        "def get_model(train_set,reload_model=True):\n",
        "  \n",
        "  model = Sequential()\n",
        "  \n",
        "  if reload_model:\n",
        "    if os.path.isfile(Config.MODEL_PATH):\n",
        "      model = load_model(Config.MODEL_PATH)\n",
        "      model.fit(train_set,train_set,batch_size=1, epochs=Config.EPOCHS, shuffle=False)\n",
        "      model.save(Config.MODEL_PATH)\n",
        "      return model\n",
        "  \n",
        "  \n",
        "  model.add(Bidirectional(LSTM(units=256, return_sequences=True,\n",
        "                          input_shape=(train_set.shape[1],train_set.shape[2] ))))\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Bidirectional(LSTM(units=128, return_sequences=True )))\n",
        "  model.add(keras.layers.GaussianNoise(0.2))\n",
        "  model.add(keras.layers.GaussianDropout(0.2))\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Dense(256,activation=\"relu\", kernel_initializer = tf.keras.initializers.GlorotUniform()))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(TimeDistributed((Dense(1,activation=\"sigmoid\", kernel_initializer = tf.keras.initializers.GlorotUniform() ,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))))\n",
        "  model.compile(loss='cosine similarity', optimizer= tf.keras.optimizers.Nadam(), metrics=[\"accuracy\"])\n",
        "  model.fit(train_set,train_set,batch_size=Config.BATCH_SIZE, epochs=60, shuffle=False)\n",
        "  model.save(Config.MODEL_PATH)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI5IUT6i00Xa"
      },
      "outputs": [],
      "source": [
        "def attention_block(inputs, time_steps):\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    a = Dense(time_steps, activation='softmax')(a)\n",
        "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
        "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
        "    return output_attention_mul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYV3ijIA0Gms"
      },
      "outputs": [],
      "source": [
        "def get_seq_model(train_set):\n",
        "  frame_features_input = keras.Input((400,512))\n",
        "  lstm_out = Bidirectional(LSTM(units=256,name=\"lstm1\", return_sequences=True, input_shape=(train_set.shape[1],train_set.shape[2]) ))(frame_features_input)\n",
        "  lstm_out = Bidirectional(LSTM(units=128, name=\"lstm2\", return_sequences=True ))(lstm_out)\n",
        "\n",
        "  lstm_out = GaussianNoise(0.2)(lstm_out)\n",
        "  lstm_out = GaussianDropout(0.1)(lstm_out)\n",
        "  lstm_out = LayerNormalization()(lstm_out)\n",
        "\n",
        "  #print(lstm_out.shape)\n",
        "  #attention_mul = attention_block(lstm_out, lstm_out.shape[1])\n",
        "  #attention_mul = GlobalAveragePooling1D()(attention_mul)\n",
        "\n",
        "  x = Dense(256,name=\"dense1\",activation=\"relu\", kernel_initializer = tf.keras.initializers.GlorotUniform() ,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(lstm_out)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x = Dense(512,activation=\"softmax\", kernel_initializer = tf.keras.initializers.GlorotUniform() ,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x)\n",
        "  \n",
        "  model = Model(frame_features_input, x)\n",
        "  model.compile(loss=CosineSimilarity(), optimizer= tf.keras.optimizers.Nadam() , metrics=[\"accuracy\"])\n",
        "  model.fit(train_set,train_set,batch_size=Config.BATCH_SIZE, epochs=Config.EPOCHS, shuffle=False)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SX1g95kYVw6"
      },
      "outputs": [],
      "source": [
        "def train_all(dataset,size):\n",
        "  indices = np.arange(len(dataset))\n",
        "  count = 0\n",
        "  np.random.shuffle(indices)\n",
        "  \n",
        "  data_seq = np.zeros((size,400,512))\n",
        "  for idx in indices:\n",
        "    seq = dataset[idx]\n",
        "    seq_split = np.vsplit(seq,[400])  \n",
        "    if seq_split[1].shape[0] != 0 :\n",
        "      seq_split[1]= np.resize(seq_split[1],(400,512))\n",
        "      data_seq[count] = seq_split[0]\n",
        "      count += 1\n",
        "      data_seq[count] = seq_split[1]\n",
        "      count += 1\n",
        "      \n",
        "    else:\n",
        "      seq_split[0]= np.resize(seq_split[0],(400,512))\n",
        "      data_seq[count] = seq_split[0]\n",
        "      count += 1\n",
        "  model = get_seq_model(data_seq)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "model=train_all(video_features,size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "VideoAugmentation_for_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}